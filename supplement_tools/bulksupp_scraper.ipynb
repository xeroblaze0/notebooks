{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ec1796",
   "metadata": {},
   "source": [
    "# BulkSupplements A-Z Scraper\n",
    "This notebook scrapes product titles and URLs from BulkSupplements A-Z page and saves them to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e64d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This finds the headers of the index\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd  # For easier CSV handling\n",
    "url = 'https://www.bulksupplements.com/pages/products-a-z'\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "az_list = soup.find('div', class_='az-list')\n",
    "if not az_list:\n",
    "    raise Exception('az-list element not found')\n",
    "az_main_wraps = az_list.find_all('div', class_='az-list-main-wrap', recursive=False)\n",
    "print(f'Number of az-list-main-wrap divs:', len(az_main_wraps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract product names, URLs, and az-list-header/az-list-header-first id from each az-list-main-wrap\n",
    "data = []\n",
    "for wrap in az_main_wraps:\n",
    "    # Find header id (az-list-header or az-list-header-first)\n",
    "    header_span = wrap.find('span', class_='az-list-header')\n",
    "    if not header_span:\n",
    "        header_span = wrap.find('span', class_='az-list-header-first')\n",
    "    header_id = None\n",
    "    if header_span and header_span.has_attr('id'):\n",
    "        header_id = header_span['id']\n",
    "        if header_id.startswith('az-'):\n",
    "            header_id = header_id[3:]\n",
    "    ul = wrap.find('ul', class_='az-list-columns')\n",
    "    if not ul:\n",
    "        continue\n",
    "    for a in ul.find_all('a'):\n",
    "        title = a.get_text(strip=True)\n",
    "        href = a.get('href')\n",
    "        if title and href and 'Capsules' not in title and 'Softgels' not in title and 'Pocket' not in title and 'Performance' not in title and 'Machine' not in title:\n",
    "            data.append({'title': title, 'url': href, 'header_id': header_id})\n",
    "print(f'Extracted {len(data)} products (excluding \"Capsules\", \"Softgels\", \"Pocket\", \"Performance\", and \"Machine\")')\n",
    "# Display first 5 entries in a readable format\n",
    "for item in data[:5]:\n",
    "    print(f\"Header ID: {item['header_id']}\")\n",
    "    print(f\"Name: {item['title']}\")\n",
    "    print(f\"URL: {item['url']}\")\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59825f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scraping for a single product URL and display only specified headers and their content\n",
    "product_url = 'https://www.bulksupplements.com/products/wheatgrass-powder-2'  # Enter a product URL here\n",
    "if product_url:\n",
    "    response = requests.get(product_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # --- Scrape Variants and Prices ---\n",
    "    variants_and_prices = []\n",
    "    variant_picker = soup.find('variant-picker')\n",
    "\n",
    "    variants = [span.get_text(strip=True) for span in variant_picker.select('.variant-picker__option-values span')]\n",
    "    print('Variants found:', variants)\n",
    "\n",
    "    # Find div with id containing 'shopify-section-template'\n",
    "    section_div = soup.find('div', id=lambda x: x and 'shopify-section-template' in x)\n",
    "    if section_div:\n",
    "        # Find x-tabs inside this div\n",
    "        x_tabs = section_div.find('x-tabs')\n",
    "        if x_tabs:\n",
    "            # Define the headers we are interested in\n",
    "            interested_headers = [\n",
    "                \"Serving Size\",\n",
    "                \"Other Ingredients\",\n",
    "                \"Allergen Information\",\n",
    "                \"Free of\",\n",
    "                \"Suggested Use\"\n",
    "            ]\n",
    "            headers_data = []\n",
    "            # Search only <div> and <p> with role='tabpanel' inside x-tabs for <b> tags\n",
    "            for tabpanel in x_tabs.find_all(['div', 'p'], attrs={'role': 'tabpanel'}):\n",
    "                # Find all <b> tags within the tabpanel\n",
    "                b_tags = tabpanel.find_all('b')\n",
    "                for b_tag in b_tags:\n",
    "                    header_text_with_colon = b_tag.get_text(strip=True)\n",
    "                    header = header_text_with_colon.strip(':')\n",
    "                    if header in interested_headers:\n",
    "                        # Find the parent <p> or <div> of the <b> tag\n",
    "                        parent_tag = b_tag.find_parent(['p', 'div'])\n",
    "                        if parent_tag:\n",
    "                            # Get the text of the parent and remove the header to get the content\n",
    "                            content = parent_tag.get_text(separator=' ', strip=True).replace(header_text_with_colon, '', 1).strip()\n",
    "                            headers_data.append({'name': header, 'url': product_url, 'content': content})\n",
    "            if headers_data:\n",
    "                print('--- Supplemental Facts ---')\n",
    "                for entry in headers_data:\n",
    "                    # print(f\"URL: {entry['url']}\")\n",
    "                    print(f\"Header: {entry['name']}\")\n",
    "                    print(f\"Content: {entry['content']}\")\n",
    "                    print('-' * 40)\n",
    "            else:\n",
    "                print('No interested headers found in tabpanel elements in x-tabs.')\n",
    "        else:\n",
    "            print('x-tabs element not found')\n",
    "    else:\n",
    "        print('shopify-section-template div not found')\n",
    "else:\n",
    "    print('Please enter a product URL in product_url.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91aa48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pprint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd  \n",
    "\n",
    "# --- Step 1: Re-extract product names, URLs, and az-list-header/az-list-header-first id from each az-list-main-wrap ---\n",
    "url = 'https://www.bulksupplements.com/pages/products-a-z'\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "az_list = soup.find('div', class_='az-list')\n",
    "if not az_list:\n",
    "    raise Exception('az-list element not found')\n",
    "az_main_wraps = az_list.find_all('div', class_='az-list-main-wrap', recursive=False)\n",
    "print(f'Number of az-list-main-wrap divs:', len(az_main_wraps))\n",
    "\n",
    "# --- Step 2: Scrape interested headers for each product in the filtered list ---\n",
    "initial_products = []\n",
    "interested_headers = [\n",
    "    \"Serving Size\",\n",
    "    \"Other Ingredients\",\n",
    "    \"Allergen Information\",\n",
    "    \"Free of\",\n",
    "    \"Suggested Use\"\n",
    "]\n",
    "for wrap in az_main_wraps:\n",
    "    # Find header id (az-list-header or az-list-header-first)\n",
    "    header_span = wrap.find('span', class_='az-list-header')\n",
    "    if not header_span:\n",
    "        header_span = wrap.find('span', class_='az-list-header-first')\n",
    "    header_id = None\n",
    "    if header_span and header_span.has_attr('id'):\n",
    "        header_id = header_span['id']\n",
    "        if header_id.startswith('az-'):\n",
    "            header_id = header_id[3:]\n",
    "    ul = wrap.find('ul', class_='az-list-columns')\n",
    "    if not ul:\n",
    "        continue\n",
    "    for a in ul.find_all('a'):\n",
    "        title = a.get_text(strip=True)\n",
    "        href = a.get('href')\n",
    "        if title and href and 'Capsules' not in title and 'Softgels' not in title and 'Pocket' not in title and 'Performance' not in title:\n",
    "            initial_products.append({'title': title, 'url': href, 'header_id': header_id})\n",
    "print(f'Extracted {len(initial_products)} products (excluding \"Capsules\", \"Softgels\", \"Pocket\", and \"Performance\")')\n",
    "\n",
    "scraped_data = []\n",
    "for product in initial_products:\n",
    "    print(f\"Scraping: {product['title']}\")\n",
    "\n",
    "    # Construct full URL\n",
    "    prod_url = product['url']\n",
    "    if not prod_url.startswith('http'):\n",
    "        prod_url = 'https://www.bulksupplements.com' + prod_url\n",
    "\n",
    "    try:\n",
    "        response = requests.get(prod_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # --- Scrape Supplemental Facts ---\n",
    "        supplemental_facts = {}\n",
    "        for header in interested_headers:\n",
    "            supplemental_facts[header] = None\n",
    "        \n",
    "        x_tabs = soup.find('x-tabs')\n",
    "        if x_tabs:\n",
    "            for tabpanel in x_tabs.find_all(['div', 'p'], attrs={'role': 'tabpanel'}):\n",
    "                b_tags = tabpanel.find_all('b')\n",
    "                for b_tag in b_tags:\n",
    "                    header = b_tag.get_text(strip=True).strip(':')\n",
    "                    if header in interested_headers:\n",
    "                        parent_tag = b_tag.find_parent(['p', 'div'])\n",
    "                        if parent_tag:\n",
    "                            content = parent_tag.get_text(separator=' ', strip=True).replace(b_tag.get_text(), '', 1).strip()\n",
    "                            supplemental_facts[header] = content\n",
    "        \n",
    "        # --- Scrape Variants ---\n",
    "        # variant_picker = soup.find('variant-picker')\n",
    "        # variants = [span.get_text(strip=True) for span in variant_picker.select('.variant-picker__option-values span')]\n",
    "\n",
    "        # --- Combine data ---\n",
    "        base_product_details = {\n",
    "            'title': product['title'],\n",
    "            'url': prod_url,\n",
    "            'header_id': product['header_id'],\n",
    "            **supplemental_facts,\n",
    "            # 'variants': variants\n",
    "        }\n",
    "\n",
    "        scraped_data.append(base_product_details)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error scraping {product['title']}: {e}\")\n",
    "        error_details = {\n",
    "            'title': product['title'],\n",
    "            'url': prod_url,\n",
    "            'header_id': product['header_id'],\n",
    "            'error': str(e)\n",
    "        }\n",
    "        scraped_data.append(error_details)\n",
    "\n",
    "    time.sleep(1)  # Be polite to the server\n",
    "\n",
    "# --- Step 3: Print the final combined data ---\n",
    "print(\"\\n--- Scraping Complete ---\")\n",
    "print(\"Final extracted data for 'A' products:\")\n",
    "pprint.pprint(scraped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Save the data to a CSV file ---\n",
    "if scraped_data:\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "    \n",
    "    # Define the desired column order\n",
    "    column_order = ['title', 'url', 'header_id'] + interested_headers\n",
    "    # Reorder columns, adding any that might be missing (like 'error')\n",
    "    df = df.reindex(columns=column_order + [col for col in df.columns if col not in column_order])\n",
    "\n",
    "    csv_filename = 'bulksupp_products_A.csv'\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"\\nSuccessfully saved {len(df)} products to {csv_filename}\")\n",
    "else:\n",
    "    print(\"\\nNo data to save to CSV.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a9eb3c",
   "metadata": {},
   "source": [
    "New Selenium Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8421818",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c2ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This scrapes prices from a product page using Selenium and Firefox WebDriver\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "# Start browser\n",
    "service = Service()\n",
    "options = Options()\n",
    "options.headless = True  # Run browser in headless mode for notebooks\n",
    "driver = webdriver.Firefox(service=service, options=options)\n",
    "\n",
    "# Open the page\n",
    "driver.get('https://www.bulksupplements.com/products/wheatgrass-powder-2')  # Replace with your URL\n",
    "\n",
    "# Find all price elements inside the container\n",
    "prices = driver.find_elements(By.CLASS_NAME, 'product-info__price')\n",
    "\n",
    "# Print prices\n",
    "for price in prices:\n",
    "    print(price.text)\n",
    "\n",
    "# Close browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9524591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This scrapes different sizes and their prices from a product page using Selenium and Firefox WebDriver\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import time\n",
    "\n",
    "service = Service()\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(service=service, options=options)\n",
    "\n",
    "driver.get('https://www.bulksupplements.com/products/wheatgrass-powder-2')\n",
    "time.sleep(2)\n",
    "\n",
    "# Step 1: Select \"Powder\" if there's a type picker\n",
    "try:\n",
    "    powder_button = driver.find_element(By.XPATH, \"//button[contains(., 'Powder')]\")\n",
    "    powder_button.click()\n",
    "    time.sleep(1)\n",
    "except Exception:\n",
    "    print(\"Powder button not found or already selected.\")\n",
    "\n",
    "# Step 2: Find the correct fieldset for \"Size:\"\n",
    "size_fieldset = None\n",
    "fieldsets = driver.find_elements(By.CSS_SELECTOR, \"fieldset.variant-picker__option\")\n",
    "for fs in fieldsets:\n",
    "    try:\n",
    "        legend = fs.find_element(By.TAG_NAME, \"legend\")\n",
    "        if legend.text.strip() == \"Size:\":\n",
    "            size_fieldset = fs\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if size_fieldset:\n",
    "    size_radios = size_fieldset.find_elements(By.CSS_SELECTOR, \"input[type='radio']\")\n",
    "    for radio in size_radios:\n",
    "        try:\n",
    "            label = size_fieldset.find_element(By.CSS_SELECTOR, f\"label[for='{radio.get_attribute('id')}']\")\n",
    "            if not label.text.strip():\n",
    "                continue  # Skip if label text is empty\n",
    "        except Exception:\n",
    "            continue  # Skip if label not found\n",
    "        driver.execute_script(\"arguments[0].click();\", radio)\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            price = driver.find_element(By.CLASS_NAME, 'product-info__price').text\n",
    "        except Exception:\n",
    "            continue  # Skip if price not found\n",
    "        print(f\"Size: {label.text} | Price: {price}\")\n",
    "else:\n",
    "    print(\"No fieldset with legend 'Size:' found.\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c359fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import time\n",
    "\n",
    "service = Service()\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(service=service, options=options)\n",
    "\n",
    "driver.get('https://www.bulksupplements.com/pages/products-a-z')\n",
    "time.sleep(2)  # Wait for page to load\n",
    "\n",
    "# Find the az-list container\n",
    "az_list = driver.find_element(By.CLASS_NAME, 'az-list')\n",
    "az_main_wraps = az_list.find_elements(By.CSS_SELECTOR, 'div.az-list-main-wrap')\n",
    "\n",
    "data = []\n",
    "for wrap in az_main_wraps:\n",
    "    # Find header id (az-list-header or az-list-header-first)\n",
    "    header_span = None\n",
    "    try:\n",
    "        header_span = wrap.find_element(By.CLASS_NAME, 'az-list-header')\n",
    "    except:\n",
    "        try:\n",
    "            header_span = wrap.find_element(By.CLASS_NAME, 'az-list-header-first')\n",
    "        except:\n",
    "            pass\n",
    "    header_id = None\n",
    "    if header_span:\n",
    "        header_id = header_span.get_attribute('id')\n",
    "        if header_id and header_id.startswith('az-'):\n",
    "            header_id = header_id[3:]\n",
    "    try:\n",
    "        ul = wrap.find_element(By.CLASS_NAME, 'az-list-columns')\n",
    "        links = ul.find_elements(By.TAG_NAME, 'a')\n",
    "        for a in links:\n",
    "            title = a.text.strip()\n",
    "            href = a.get_attribute('href')\n",
    "            if title and href and all(x not in title for x in ['Capsules', 'Softgels', 'Pocket', 'Performance', 'Machine']):\n",
    "                data.append({'title': title, 'url': href, 'header_id': header_id})\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f'Extracted {len(data)} products (excluding unwanted types)')\n",
    "for item in data[:1]:\n",
    "    print(f\"Header ID: {item['header_id']}\")\n",
    "    print(f\"Name: {item['title']}\")\n",
    "    print(f\"URL: {item['url']}\")\n",
    "    print('-' * 40)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123189f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 480 products (excluding unwanted types)\n",
      "Scraped: 5-HTP (Time: 14.22 seconds)\n",
      "\n",
      "Successfully saved 480 products to bulksupp_products.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "interested_headers = [\n",
    "    \"Serving Size\",\n",
    "    \"Other Ingredients\",\n",
    "    \"Allergen Information\",\n",
    "    \"Free of\",\n",
    "    \"Suggested Use\"\n",
    "]\n",
    "\n",
    "service = Service()\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Firefox(service=service, options=options)\n",
    "\n",
    "# Step 1: Scrape product URLs from A-Z page\n",
    "driver.get('https://www.bulksupplements.com/pages/products-a-z')\n",
    "time.sleep(2)\n",
    "\n",
    "# Find the az-list container\n",
    "az_list = driver.find_element(By.CLASS_NAME, 'az-list')\n",
    "az_main_wraps = az_list.find_elements(By.CSS_SELECTOR, 'div.az-list-main-wrap')\n",
    "\n",
    "data = []\n",
    "for wrap in az_main_wraps:\n",
    "    # Find header id (az-list-header or az-list-header-first)\n",
    "    header_span = None\n",
    "    try:\n",
    "        header_span = wrap.find_element(By.CLASS_NAME, 'az-list-header')\n",
    "    except:\n",
    "        try:\n",
    "            header_span = wrap.find_element(By.CLASS_NAME, 'az-list-header-first')\n",
    "        except:\n",
    "            pass\n",
    "    header_id = None\n",
    "    if header_span:\n",
    "        header_id = header_span.get_attribute('id')\n",
    "        if header_id and header_id.startswith('az-'):\n",
    "            header_id = header_id[3:]\n",
    "    try:\n",
    "        ul = wrap.find_element(By.CLASS_NAME, 'az-list-columns')\n",
    "        links = ul.find_elements(By.TAG_NAME, 'a')\n",
    "        for a in links:\n",
    "            title = a.text.strip()\n",
    "            href = a.get_attribute('href')\n",
    "            if title and href and all(x not in title for x in ['Capsules', 'Softgels', 'Pocket', 'Performance', 'Machine']):\n",
    "                data.append({'title': title, 'url': href, 'header_id': header_id})\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f'Extracted {len(data)} products (excluding unwanted types)')\n",
    "\n",
    "# Step 2: Visit each product and extract interested headers\n",
    "results = []\n",
    "for idx, item in enumerate(data):  # Limit for demo; remove [:5] for all products\n",
    "    start_time = time.time()  # Start timer\n",
    "    url = item['url']\n",
    "    title = item['title']\n",
    "    # print(f\"Scraping: {title}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    entry = {}\n",
    "    \n",
    "    # Supplemental Facts Extraction\n",
    "    try:\n",
    "        # Find and click the \"Supplemental Facts\" tab\n",
    "        tabs = driver.find_elements(By.CSS_SELECTOR, \"button[role='tab']\")\n",
    "        for tab in tabs:\n",
    "            if \"Supplemental Facts\" in tab.text:\n",
    "                tab.click()\n",
    "                time.sleep(1)\n",
    "                break\n",
    "\n",
    "        # Find the active tabpanel\n",
    "        tabpanels = driver.find_elements(By.CSS_SELECTOR, \"[role='tabpanel']\")\n",
    "        for tabpanel in tabpanels:\n",
    "            if tabpanel.is_displayed():\n",
    "                panel_text = tabpanel.text\n",
    "                # Match interested headers and extract their content\n",
    "                for header in interested_headers:\n",
    "                    if header in panel_text:\n",
    "                        # Find the header and extract the following text\n",
    "                        lines = panel_text.split('\\n')\n",
    "                        for i, line in enumerate(lines):\n",
    "                            if line.strip().startswith(header):\n",
    "                                # Get the content after the header (remove header and colon)\n",
    "                                content = line.replace(header, '', 1).replace(':', '', 1).strip()\n",
    "                                if not content and i + 1 < len(lines):\n",
    "                                    content = lines[i + 1].strip()\n",
    "                                entry[header] = content\n",
    "                break  # Only process the first visible tabpanel\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "\n",
    "    data[idx].update(entry)\n",
    "\n",
    "    # Pricing Information \n",
    "    # Select \"Powder\" if there's a type picker\n",
    "    try:\n",
    "        powder_button = driver.find_element(By.XPATH, \"//button[contains(., 'Powder')]\")\n",
    "        powder_button.click()\n",
    "        time.sleep(1)\n",
    "    except Exception:\n",
    "        # print(\"Powder button not found or already selected.\")\n",
    "        pass\n",
    "\n",
    "    variations = []\n",
    "\n",
    "    # Step 2: Find the correct fieldset for \"Size:\"\n",
    "    size_fieldset = None\n",
    "    fieldsets = driver.find_elements(By.CSS_SELECTOR, \"fieldset.variant-picker__option\")\n",
    "    for fs in fieldsets:\n",
    "        try:\n",
    "            legend = fs.find_element(By.TAG_NAME, \"legend\")\n",
    "            if legend.text.strip() == \"Size:\":\n",
    "                size_fieldset = fs\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if size_fieldset:\n",
    "        size_radios = size_fieldset.find_elements(By.CSS_SELECTOR, \"input[type='radio']\")\n",
    "        for radio in size_radios:\n",
    "            try:\n",
    "                label = size_fieldset.find_element(By.CSS_SELECTOR, f\"label[for='{radio.get_attribute('id')}']\")\n",
    "                if not label.text.strip():\n",
    "                    continue  # Skip if label text is empty\n",
    "            except Exception:\n",
    "                continue  # Skip if label not found\n",
    "            driver.execute_script(\"arguments[0].click();\", radio)\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                price = driver.find_element(By.CLASS_NAME, 'product-info__price').text.strip(\"Sale price\")\n",
    "            except Exception:\n",
    "                continue  # Skip if price not found\n",
    "            # print(f\"Size: {label.text} | Price: {price}\")\n",
    "            variations.append({'size': label.text, 'price': price})\n",
    "        # print(\"Variations:\")\n",
    "        # for var in variations:\n",
    "        #     print(f\"    Size: {var['size']} | Price: {var['price']}\")\n",
    "        # entry['variations'] = variations\n",
    "    else:\n",
    "        print(\"No fieldset with legend 'Size:' found.\")\n",
    "\n",
    "    data[idx].update({'Pricing': variations})\n",
    "\n",
    "    print(f\"Scraped: {title} (Time: {(time.time() - start_time):.2f} seconds)\")\n",
    "    # print(\"Title: \", title)\n",
    "    # print(\"Entry:\", entry)\n",
    "    # print(\"Variations:\", variations)\n",
    "    # print()\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# --- Step 4: Save the data to a CSV file ---\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the desired column order\n",
    "column_order = ['title', 'url', 'header_id'] + interested_headers\n",
    "# Reorder columns, adding any that might be missing (like 'error')\n",
    "df = df.reindex(columns=column_order + [col for col in df.columns if col not in column_order])\n",
    "\n",
    "csv_filename = 'bulksupp_products.csv'\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nSuccessfully saved {len(df)} products to {csv_filename}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
